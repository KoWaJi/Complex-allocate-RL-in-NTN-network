from datetime import datetime  # æ—¥æœŸæ—¶é—´å¯¹è±¡ï¼Œå¸¸ç”¨çš„å±æ€§æœ‰yearã€monthã€day
from time import time          # æ—¶é—´å¯¹è±¡

import numpy as np
import torch

USE_CUDA = torch.cuda.is_available()
FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor


def to_numpy(var):
    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()


def to_tensor(ndarray, requires_grad=False, dtype=FLOAT):     # ä¸éœ€è¦ä¿å­˜æ¢¯åº¦å¯ä»¥å®šä¹‰ä¸ºFalseï¼Œä¸ºäº†ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œé»˜è®¤çš„tensoræ˜¯ä¸éœ€è¦æ¢¯åº¦çš„
    t = torch.from_numpy(ndarray)    # ä»numpy.ndarrayåˆ›å»ºä¸€ä¸ªå¼ é‡ï¼Œè¿”å›çš„å¼ é‡å’Œndarrayå…±äº«ä¸€ä¸ªå†…å­˜ï¼Œå¼ é‡çš„ä¿®æ”¹ä¼šåæ˜ åœ¨ndarryä¸­
    t.requires_grad_(requires_grad)  # æ˜¯é€šç”¨æ•°æ®ç»“æ„Tensorçš„ä¸€ä¸ªå±æ€§ï¼Œç”¨äºè¯´æ˜å½“å‰é‡æ˜¯å¦éœ€è¦åœ¨è®¡ç®—ä¸­ä¿ç•™å¯¹åº”çš„æ¢¯åº¦ä¿¡æ¯
    if USE_CUDA:
        return t.type(dtype).to(torch.cuda.current_device())  # å°†æ¨¡å‹åŠ è½½åˆ°to()å†…çš„è®¾å¤‡ä¸­
    else:
        return t.type(dtype)


def soft_update_inplace(target, source, tau):  # tau=2ğŸ¥§
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - tau) + param.data * tau
        )


def hard_update_inplace(target, source):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)


def add_param_noise_inplace(target, std=0.01):
    for target_param in list(target.parameters()):
        d = np.random.randn(1)
        d = d * std
        d = to_tensor(d, requires_grad=False)
        target_param.data.add_(d)


def get_current_time_str():
    return datetime.now().strftime("%Y-%B-%d-%H-%M-%S")


def counted(f):  # ä¸@ä¹‹åçš„å‡½æ•°åä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯counted
    def wrapped(self, *args):  # wrappedæ˜¯è£…é¥°å™¨
        self.n_step += 1
        return f(self, *args)

    return wrapped


def timed(f):
    def wrapped(self, *args):
        ts = time()
        result = f(self, *args)
        te = time()
        print('%s func:%r took: %2.4f sec' % (self, f.__name__, te - ts))
        return result

    return wrapped


if __name__ == '__main__':
    print(USE_CUDA)
    # print(torch.normal(2, 3, size=(1, 1)))
    print(np.random.randn(1))
